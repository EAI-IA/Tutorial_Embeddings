{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtiE5a2Bu5Bw69nKxlBGty",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EAI-IA/Tutorial_Embeddings/blob/main/Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28.1\n",
        "!pip install pandas\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "cyawv6lw5144"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8wDIvcG64UVQ"
      },
      "outputs": [],
      "source": [
        "# Importamos todas las dependencias requeridas, en este caso será Gradio para desarrollar la interfaz grafica y openai para realizar los llamados a su API\n",
        "import gradio as gr\n",
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "from openai.embeddings_utils import get_embedding\n",
        "from openai.embeddings_utils import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key=\"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\""
      ],
      "metadata": {
        "id": "SM3mUeuL5VfK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palabras = [\"casa\", \"perro\", \"Carlos\", \".\", \"leon\", \"lobo\", \"tigew\", \"zebra\"]"
      ],
      "metadata": {
        "id": "PdXUjw1g5adP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diccionario = {}\n",
        "for i in palabras:\n",
        "  diccionario[i] = get_embedding(i ,engine=\"text-embedding-ada-002\")"
      ],
      "metadata": {
        "id": "1Weisrge5cxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diccionario.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaVvE9Fo_o6L",
        "outputId": "9a43d79d-f7bc-4b8e-8f0f-79e14469668d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['casa', 'perro', 'Carlos', '.', 'leon', 'lobo', 'tigew', 'zebra'])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El codigo siguiente esta buscando en el diccionario la palabra que queremos desarrollar, y retornado los 10 representaciones numericas de la palabra leon, para que finalmente te retorne una valor multidimensional de 1536.\n",
        "Esas representaciones sería los embeddings de cada palabra."
      ],
      "metadata": {
        "id": "r4bdzAqZAZWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palabra = \"leon\"\n",
        "print(\"Primeros 10 valores de {}:\\n\".format(palabra), diccionario[palabra][:10])\n",
        "print(\"\\n\")\n",
        "print(\"Número de dimensiones del dato embebido\\n\", len(diccionario[palabra]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrueUW_Y_vRm",
        "outputId": "03bc2a33-9a7a-48de-9224-566b0701d91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeros 10 valores de leon:\n",
            " [0.023633645847439766, 0.006025627255439758, 0.013728906400501728, -0.035897355526685715, -0.02209518663585186, 0.015999963507056236, -0.018651971593499184, 0.0008937708334997296, -0.013509126380085945, -0.015399232506752014]\n",
            "\n",
            "\n",
            "Número de dimensiones del dato embebido\n",
            " 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COMPARAR DOS EMBEDDINGS**\n",
        "\n",
        "\n",
        "*   Lo que contiene n_palabra_embed >> es hacer el embedding de la palabra que queremos hacer.\n",
        "*   Cosine_similarity es la aproximacion de dos vectores, en este caso la comparación del resultado de ambos diccionarios que contienen 1 palabra >> solo miden la distancia.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tm3FrQWlBLAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_palabra = \"gato\" # Palabra nueva a comparar\n",
        "palabra_comparar = \"perro\" # Palabra del diccionario con la que compararemos la nueva palabra\n",
        "n_palabra_embed = get_embedding(n_palabra, engine=\"text-embedding-ada-002\")\n",
        "similitud = cosine_similarity(diccionario[palabra_comparar], n_palabra_embed)\n",
        "print(similitud)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h55x8LeBIkH",
        "outputId": "e3adc18b-bbd5-4bfb-8733-8391c4b91dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8938930733213675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suma dos listas usando pandas\n",
        "print(\"RESULTADO DE SUMA DE 1536 verctores\")\n",
        "sumados = (pd.DataFrame(diccionario[\"leon\"])) + (pd.DataFrame(diccionario[\"zebra\"]))\n",
        "len(sumados)\n",
        "print (\"leon+zebra: \", sumados)\n",
        "\n",
        "print(\"SIMILARIDAD DE OPERACIÓN DE EMBEDDINGS E COMPARAÇÃO COM EMBEDDINGS ESPECIFICOS \")\n",
        "for key, value in diccionario.items():\n",
        "    print(key, \":\", cosine_similarity(diccionario[key], sumados))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHEGSiS_DV17",
        "outputId": "3b206958-3004-4802-9b01-fb83b892efbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESULTADO DE SUMA DE 1536 verctores\n",
            "leon+zebra:               0\n",
            "0    -0.007573\n",
            "1    -0.009127\n",
            "2     0.001552\n",
            "3    -0.053463\n",
            "4    -0.034278\n",
            "...        ...\n",
            "1531  0.060241\n",
            "1532 -0.027278\n",
            "1533 -0.028748\n",
            "1534  0.018847\n",
            "1535 -0.048257\n",
            "\n",
            "[1536 rows x 1 columns]\n",
            "SIMILARIDAD DE OPERACIÓN DE EMBEDDINGS E COMPARAÇÃO COM EMBEDDINGS ESPECIFICOS \n",
            "casa : [0.82711427]\n",
            "perro : [0.8498726]\n",
            "Carlos : [0.84014643]\n",
            ". : [0.8451586]\n",
            "leon : [0.95318831]\n",
            "lobo : [0.85606585]\n",
            "tigew : [0.85960203]\n",
            "zebra : [0.95318832]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONVERSACIÓN CON OPENAI**\n",
        "\n",
        "\n",
        "text.csv >>> es nua archivo de respuestas que puede dar el chatbot. (es demostrativo debido a que es de prueba)\n",
        "\n",
        "embeddings.csv >> es el relacionamiento del texto anterior con embeddings\n",
        "\n",
        "De esta forma al hacer una pregunta, se hace el embeddings de la requisición que se desea y está retornando los 5 vectores mas cercanos a la pregunta (embedding de la prgunta).\n",
        "\n",
        "Encontrando un similitud de semantica.\n"
      ],
      "metadata": {
        "id": "aocKN8htSz2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_text(path=\"texto.csv\"):\n",
        "    conocimiento_df = pd.read_csv(path)\n",
        "    conocimiento_df['Embedding'] = conocimiento_df['texto'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
        "    conocimiento_df.to_csv('embeddings.csv')\n",
        "    return conocimiento_df\n",
        "\n",
        "def buscar(busqueda, datos, n_resultados=5):\n",
        "    busqueda_embed = get_embedding(busqueda, engine=\"text-embedding-ada-002\")\n",
        "    datos[\"Similitud\"] = datos['Embedding'].apply(lambda x: cosine_similarity(x, busqueda_embed))\n",
        "    datos = datos.sort_values(\"Similitud\", ascending=False)\n",
        "    return datos.iloc[:n_resultados][[\"texto\", \"Similitud\", \"Embedding\"]]\n",
        "\n",
        "texto_emb = embed_text(\"./chatbot_qa.csv\")\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    busqueda = gr.Textbox(label=\"Buscar\")\n",
        "    output = gr.DataFrame(headers=['texto'])\n",
        "    greet_btn = gr.Button(\"Preguntar\")\n",
        "    greet_btn.click(fn=buscar, inputs=[busqueda, gr.DataFrame(texto_emb)], outputs=output)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "DDkemBhdSeMU",
        "outputId": "1c388867-27b1-4e8d-feb1-1eda3ccf0a64"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://885f0d6d04937f4eba.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://885f0d6d04937f4eba.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain pypdf\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(\"./mtg.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "SYSiAWIRx227"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Un elemento por cada página\n",
        "pages[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aDri2z1Az1ac",
        "outputId": "8dad1134-e332-4b46-aa00-e6b67a7f9abc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Guía de inicio \\nrápidoEdad: 13 o +'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El Separador, sirve para recortar segmentos del texto, en este caso CharacterSplitter convierte el texto en parrafos, esta buscando \"./n\" un punto y aparte para realizar chunks, con un tamaño de 800 de tamaño."
      ],
      "metadata": {
        "id": "hcwbOSyj5sM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objeto que va a hacer los cortes en el texto\n",
        "split = CharacterTextSplitter(chunk_size=800, separator = '.\\n')"
      ],
      "metadata": {
        "id": "m_qDqC1Wz-3-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textos = split.split_documents(pages) # Lista de textos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjpf86Nv0B3z",
        "outputId": "dc1345e3-0ba0-4133-8601-1907b1d1ce9e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1507, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 892, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1311, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1139, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 916, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 910, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1074, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 867, which is longer than the specified 800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Jsg3vJ876vFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(textos[0])\n",
        "print(textos[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-54JR55N6e0r",
        "outputId": "ff0f1f33-4c17-4587-a4bf-bd050e21b44b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guía de inicio \n",
            "rápidoEdad: 13 o +\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Textos >> esta asociado al número de la parrafo al que quieres apuntar, en este caso asigna a todos los parrafos en una lista de DataFrame, que tiene una estructura muy parecida al ejercicio de respuestas dadas inicialmente desde el chatbot original.\n",
        "y esto es guardado en un DataFrame de Pandas."
      ],
      "metadata": {
        "id": "7XPaBYS97PS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraemos la parte de page_content de cada texto y lo pasamos a un dataframe\n",
        "textos = [str(i.page_content) for i in textos] #Lista de parrafos\n",
        "parrafos = pd.DataFrame(textos, columns=[\"texto\"])\n",
        "print(parrafos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ596cT57GDp",
        "outputId": "50f6e78a-cc41-4b94-dfca-5ed3ddd0adfb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                texto\n",
            "0                 Guía de inicio \\nrápidoEdad: 13 o +\n",
            "1   2Bienvenido a Magic: The Gathering, el mejor j...\n",
            "2   Hay miles de cartas para elegir, por lo \\nque ...\n",
            "3   Siempre que ganes vidas, puedes \\nponer un con...\n",
            "4   Un símbolo de convicción en tiempos \\nincierto...\n",
            "..                                                ...\n",
            "60  toque mortal : una criatura que recibe daño de...\n",
            "61  X, oX: algunos hechizos y habilidades tienen \\...\n",
            "62  Fase inicial\\n ∙Paso de enderezar: vuelve a po...\n",
            "63  ∙ Paso de declarar bloqueadoras: cada criatura...\n",
            "64  ∙Paso de final del combate.\\nFase principal (d...\n",
            "\n",
            "[65 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parrafos['Embedding'] = parrafos[\"texto\"].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002')) # Nueva columna con los embeddings de los parrafos\n",
        "parrafos.to_csv('MTG.csv')"
      ],
      "metadata": {
        "id": "j6Dvi5Yz7-H7"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La misma funcion del chatbot de pregunts y respuestas\n",
        "def embed_text(path=\"texto.csv\"):\n",
        "    conocimiento_df = pd.read_csv(path)\n",
        "    conocimiento_df['Embedding'] = conocimiento_df['texto'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
        "    conocimiento_df.to_csv('mtg-embeddings.csv')\n",
        "    return conocimiento_df\n",
        "\n",
        "def buscar(busqueda, datos, n_resultados=5):\n",
        "    busqueda_embed = get_embedding(busqueda, engine=\"text-embedding-ada-002\")\n",
        "    datos[\"Similitud\"] = datos['Embedding'].apply(lambda x: cosine_similarity(x, busqueda_embed))\n",
        "    datos = datos.sort_values(\"Similitud\", ascending=False)\n",
        "    return datos.iloc[:n_resultados][[\"texto\", \"Similitud\", \"Embedding\"]]\n",
        "\n",
        "texto_emb = parrafos\n",
        "with gr.Blocks() as demo:\n",
        "    busqueda = gr.Textbox(label=\"Buscar\")\n",
        "    output = gr.DataFrame(headers=['texto'])\n",
        "    greet_btn = gr.Button(\"Preguntar\")\n",
        "    greet_btn.click(fn=buscar, inputs=[busqueda, gr.DataFrame(texto_emb)], outputs=output)\n",
        "\n",
        "demo.launch()\n",
        "\n",
        "\n",
        "# resp = buscar(\"Con cuanta vida empiezo?\", parrafos, 5) # Reutilizamos la funcion de \"buscar\" del app de gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "8-iWrrg3_IaY",
        "outputId": "c1d46ade-9094-474c-8cb0-24151108276d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://08a285e014f21c82c1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://08a285e014f21c82c1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resp = buscar(\"Con cuanta vida empiezo?\", parrafos, 5) # Reutilizamos la funcion de \"buscar\" del app de gradio\n",
        "print(resp.texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9GtIN5lAbRb",
        "outputId": "a4b4aae8-913b-44e7-ccd8-2de1579da1e4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8     Jasper SandnerC 214/ 269 2/1Oo2\\n™ & © 2014 Wi...\n",
            "3     Siempre que ganes vidas, puedes \\nponer un con...\n",
            "13    Cuando el Favor divino entre al \\ncampo de bat...\n",
            "0                   Guía de inicio \\nrápidoEdad: 13 o +\n",
            "7     Cuando el Favor divino entre al \\ncampo de bat...\n",
            "Name: texto, dtype: object\n"
          ]
        }
      ]
    }
  ]
}